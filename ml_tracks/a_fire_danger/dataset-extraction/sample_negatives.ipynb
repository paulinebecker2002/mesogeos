{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e23bad-13a9-4a2a-9382-e14db2b57c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from shapely import wkt\n",
    "import shapely\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebaa9c6-d06e-4533-8ce2-b42a2164ace1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADD here path to the datacube\n",
    "ds = xr.open_zarr('/hkfs/work/workspace/scratch/uyxib-pauline_gddpfa/mesogeos/mesogeos_cube.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9d617e-f374-4ba5-b25e-8f272113b0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ADD here path to the biomes downloaded from \"\"\n",
    "biome = gpd.read_file(\"/hkfs/work/workspace/scratch/uyxib-pauline_gddpfa/mesogeos/auxilliary/ecoregions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61b41dd-291b-4397-b254-f91d0260b008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biome=biome[(biome['BIOME_NUM']== 12.0) & (biome['REALM']=='Palearctic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f120fb1b-8108-4f99-9238-7a8cfe4919a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lc_1 = ds['lc_agriculture'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_2 = ds['lc_forest'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_3 = ds['lc_grassland'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_4 = ds['lc_settlement'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_5 = ds['lc_shrubland'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_6 = ds['lc_sparse_vegetation'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_7 = ds['lc_water_bodies'].sel(time=slice('2021-01-01', '2021-01-01'))[0]\n",
    "lc_8 = ds['lc_wetland'].sel(time=slice('2021-01-01', '2021-01-01'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bdffa38-31d7-4416-a379-4230b907e0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lc = np.stack([lc_1, lc_2, lc_3, lc_4, lc_5, lc_6, lc_7, lc_8])\n",
    "lc = np.argmax(lc, axis=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8413d08e-8e2b-43c1-8df6-0da1a403145a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ADD here the path to the positive csv file\n",
    "gpd = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9065aab2-0a4b-41ef-92c5-ba643508ec7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpd = gpd[gpd['time_idx'] == 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e81368-7072-44bc-8528-4ab8159cd83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lc_burned = gpd[['lc_agriculture', 'lc_forest', 'lc_grassland', 'lc_settlement', 'lc_shrubland', 'lc_sparse_vegetation', 'lc_water_bodies', 'lc_wetland']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf8ed99-c42a-45c6-a4cb-6d2642bc315a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lc_burned = np.argmax(lc_burned.to_numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9749a2-330e-4826-a306-6c025638886a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "clc_counter = Counter(lc)\n",
    "burned_clc_counter = Counter(lc_burned)\n",
    "burned_sum = 0\n",
    "for k, v in clc_counter.items():\n",
    "    if (\n",
    "        (burned_clc_counter.get(k) is None) # if never burned\n",
    "    or (burned_clc_counter.get(k) < 50) # if rarely burned\n",
    "    or (k in [6,7]) # if cannot be burned\n",
    "    ):\n",
    "        burned_clc_counter[k] = 0\n",
    "    else:\n",
    "        burned_sum += burned_clc_counter[k]\n",
    "\n",
    "p = np.array([burned_clc_counter[x]/(clc_counter[x]*burned_sum) for x in list(lc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a580b6f-f728-4b95-acae-283ccf5b8447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpd['time'] = pd.to_datetime(gpd['time'])\n",
    "gpd['YEAR'] = gpd['time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4171b10-303d-4ccb-8005-56e3260f590f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positives_per_year = gpd.groupby('YEAR').count()['time'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12b6b99-84f8-4c07-8b8b-4b668fd4d631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "negatives_per_year = {}\n",
    "negatives_per_day = defaultdict(dict)\n",
    "\n",
    "for k, v in positives_per_year.items():\n",
    "    negatives_per_year[k] = positives_per_year[k] * 2\n",
    "\n",
    "for year in negatives_per_year:\n",
    "    if year != 2006 and year!=2022:\n",
    "        negatives_per_day[year]['JF'] = (negatives_per_year[year] // 20)/60\n",
    "        negatives_per_day[year]['MS'] = (9*negatives_per_year[year] // 10)/240\n",
    "        negatives_per_day[year]['ND'] = (negatives_per_year[year] // 20)/60\n",
    "    elif year == 2006:\n",
    "        negatives_per_day[year]['JF'] = 0\n",
    "        negatives_per_day[year]['MS'] = (9.5*negatives_per_year[year] // 10)/210\n",
    "        negatives_per_day[year]['ND'] = (negatives_per_year[year] // 40)/60\n",
    "    elif year == 2022:\n",
    "        negatives_per_day[year]['JF'] = (negatives_per_year[year] // 40)/60\n",
    "        negatives_per_day[year]['MS'] = (9.5*negatives_per_year[year] // 10)/210\n",
    "        negatives_per_day[year]['ND'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cff28-9b6a-4e79-844b-eb0fcedbc431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1159/5996 [1:23:18<7:47:24,  5.80s/it] "
     ]
    }
   ],
   "source": [
    "#ADDD here the path to save the data\n",
    "save_dir = \"\"\n",
    "dem = ds['dem'].load()\n",
    "len_x = len(ds['x'])\n",
    "len_y = len(ds['y'])\n",
    "s_cl=0\n",
    "s_seg=0\n",
    "lag = 30\n",
    "patch_size = 5\n",
    "patch_half = 125//2\n",
    "year = 2005\n",
    "\n",
    "for t in tqdm(range(lag, len(ds['time']))):\n",
    "    t_time = pd.to_datetime(ds['time'][t].values) \n",
    "    month = pd.DatetimeIndex([ds['time'][t].values]).month[0]\n",
    "    y = str(year)\n",
    "    year = pd.DatetimeIndex([ds['time'][t].values]).year[0]\n",
    "\n",
    "    if y != str(year):\n",
    "        d_st = {}\n",
    "        ds_vars = list(ds.keys()) \n",
    "        for var in ds_vars:\n",
    "            if var == 'population' or 'lc' in var:\n",
    "                if str(year) == '2006':\n",
    "                    d_st[var] = ds[var].sel(time=slice('2006-04-01', '2006-04-01'))[0].load()\n",
    "                else:\n",
    "                    dt = str(year) + '-01-01'\n",
    "                    d_st[var] = ds[var].sel(time=slice(dt, dt))[0].load()\n",
    "                        \n",
    "    x = random.uniform(0,1)\n",
    "    if month <= 2:\n",
    "        num_samples, d = divmod(negatives_per_day[year]['JF'], 1)\n",
    "    elif month >=3 and month <=10:\n",
    "        num_samples, d = divmod(negatives_per_day[year]['MS'], 1)\n",
    "    else:\n",
    "        num_samples, d = divmod(negatives_per_day[year]['ND'], 1)\n",
    "    if x < d:\n",
    "        num_samples+=1\n",
    "    ds_tmp = ds.sel(time=slice(t_time, t_time)).load()\n",
    "    for i in range(int(num_samples)):\n",
    "        np_var = {}\n",
    "        idx = np.random.choice(np.arange(0,len_x*len_y), p=p) \n",
    "        y_idx = idx // len_x\n",
    "        x_idx = idx % len_x\n",
    "        point = shapely.geometry.Point(ds['dem']['x'][x_idx].values, ds['dem']['y'][y_idx].values)\n",
    "        while ((x_idx - patch_half < 0) or (x_idx + patch_half + 1 >= len_x) or (y_idx - patch_half< 0) or (y_idx + patch_half + 1 >= len_y)) or \\\n",
    "        (1 in ds_tmp.isel(x=slice(x_idx - patch_half,  x_idx + patch_half + 1), y=slice(y_idx - patch_half, y_idx + patch_half + 1))['burned_areas']) or \\\n",
    "        (True not in point.within(biome.geometry).tolist()) or (pd.isnull(ds_tmp.isel(x=x_idx, y=y_idx)['t2m'])):\n",
    "            idx = np.random.choice(np.arange(0,len_x*len_y), p=p) \n",
    "            y_idx = idx // len_x\n",
    "            x_idx = idx % len_x\n",
    "            point = shapely.geometry.Point(ds['dem']['x'][x_idx].values, ds['dem']['y'][y_idx].values)\n",
    " \n",
    "        x = str(dem.isel(x=x_idx)['x'].values)\n",
    "        y = str(dem.isel(y=y_idx)['y'].values)\n",
    "       \n",
    "        ign_date_str  = (t_time).strftime('%Y-%m-%d')\n",
    "        ign_date_lag_str = (t_time - pd.Timedelta(days=lag-1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        neg_sample_ds = ds.sel(time=slice(ign_date_lag_str, ign_date_str))\n",
    "\n",
    "        neg_sample_ds = neg_sample_ds.isel(x=slice(x_idx - patch_half,x_idx + patch_half + 1),\n",
    "                                  y=slice(y_idx - patch_half,y_idx + patch_half + 1))\n",
    "\n",
    "        neg_sample_ds_vars = list(neg_sample_ds.keys()) \n",
    "        for var in neg_sample_ds_vars:\n",
    "            if var == 'population' or 'lc' in var:\n",
    "                del neg_sample_ds[var]\n",
    "                neg_sample_ds[var] = d_st[var].isel(x=slice(x_idx - patch_half,x_idx + patch_half + 1),\n",
    "                                          y=slice(y_idx - patch_half,y_idx + patch_half + 1)) \n",
    "        \n",
    "        del neg_sample_ds['spatial_ref']\n",
    "        neg_sample_ds = neg_sample_ds.load()\n",
    "      \n",
    "        neg_sample_ds = neg_sample_ds.isel(x=patch_half, y=patch_half)\n",
    "        neg_sample_ds['burned_area_has'] = float(0) \n",
    "        if s_cl == 0: \n",
    "            df = neg_sample_ds.to_dataframe()\n",
    "            df['time_idx'] = np.arange(0,lag)\n",
    "            df['sample'] = s_cl\n",
    "        else:\n",
    "            df1 = neg_sample_ds.to_dataframe()\n",
    "            df1['time_idx'] = np.arange(0,lag)\n",
    "            df1['sample'] = s_cl\n",
    "            df = pd.concat([df, df1], axis=0)\n",
    "            del df1\n",
    "        s_cl+=1\n",
    "        del neg_sample_ds\n",
    "        gc.collect()\n",
    "path_df = save_dir / 'negatives.csv'\n",
    "df.to_csv(path_df)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2d47c-b2d6-40cd-aa2b-4b566a860539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
